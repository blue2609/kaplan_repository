source('C:/Users/Public/Documents/R Scripts/DATA4800_R_RF_Iris.R')
install.packages("e1071")
source('C:/Users/Public/Documents/R Scripts/DATA4800_R_RF_Iris.R')
source('C:/Users/Public/Documents/R Scripts/DATA4800_R_PCA.R')
source('C:/Users/Public/Documents/R Scripts/DATA4800_R_PCA.R')
library(ggplot2)
library(cowplot) # required to arrange multiple plots in a grid
theme_set(theme_bw(base_size=12)) # set default ggplot2 theme
library(dplyr)
library(grid) # required to draw arrows
library(datasets)
data(iris)
head(iris)
#If we want to find out which characteristics are most distinguishing between iris plants,
#we have to make many individual plots and hope we can see distinguishing patterns:
p1 <- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) + geom_point()
p2 <- ggplot(iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) + geom_point()
p3 <- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point()
p4 <- ggplot(iris, aes(x=Sepal.Width, y=Petal.Width, color=Species)) + geom_point()
plot_grid(p1, p2, p3, p4, labels = "AUTO")
#In this particular case, it seems that petal length and petal width
#are most distinct for the three species. Principal Components Analysis (PCA) allows us to systematically discover such patterns, and it works also when there are many more variables than just four.
#The basic steps in PCA are to (i) prepare a data frame that holds only the numerical columns of interest, (ii) scale the data to 0 mean and unit variance, and (iii) do the PCA with the function prcomp():
iris %>% select(-Species) %>% # remove Species column
scale() %>%                 # scale to 0 mean and unit variance
prcomp() ->                 # do PCA
pca                         # store result as `pca`
# now display the results from the PCA analysis
pca
#The main results from PCA are the standard deviations and the rotation matrix. We will talk about them below. First, however, let?s plot the data in the principal components. Specifically, we will plot PC2 vs. PC1.
#The rotated data are available as pca$x:
head(pca$x)
#As we can see, these data don?t tell us to which species which observation belongs.
#We have to add the species information back in:
# add species information back into PCA data
pca_data <- data.frame(pca$x, Species=iris$Species)
head(pca_data)
ggplot(pca_data, aes(x=PC1, y=PC2, color=Species)) + geom_point()
#In the PC2 vs PC1 plot, versicolor and virginica are much better separated.
#Next, let?s look at the rotation matrx:
pca$rotation
#It tells us how much each variable contributes to each principal component. For example, Sepal.Width contributes little to PC1 but makes up much of PC2. Often it is helpful to plot the rotation matrix as arrows. This can be done as follows:
# capture the rotation matrix in a data frame
rotation_data <- data.frame(pca$rotation, variable=row.names(pca$rotation))
# define a pleasing arrow style
arrow_style <- arrow(length = unit(0.05, "inches"),
type = "closed")
# now plot, using geom_segment() for arrows and geom_text for labels
ggplot(rotation_data) +
geom_segment(aes(xend=PC1, yend=PC2), x=0, y=0, arrow=arrow_style) +
geom_text(aes(x=PC1, y=PC2, label=variable), hjust=0, size=3, color='red') +
xlim(-1.,1.25) +
ylim(-1.,1.) +
coord_fixed() # fix aspect ratio to 1:1
#We can now see clearly that Petal.Length, Petal.Width, and Sepal.Length all contribute to PC1, and Sepal.Width dominates PC2.
#Finally, we want to look at the percent variance explained.
#The prcomp() function gives us standard deviations (stored in pca$sdev).
#To convert them into percent variance explained, we square them and then divide by the sum over all squared standard deviations:
percent <- 100*pca$sdev^2/sum(pca$sdev^2)
percent
#The first component explains 73% of the variance, the second 23%, the third 4% and the last 0.5%. We can visualize these results nicely
#in a bar chart:
perc_data <- data.frame(percent=percent, PC=1:length(percent))
ggplot(perc_data, aes(x=PC, y=percent)) +
geom_bar(stat="identity") +
geom_text(aes(label=round(percent, 2)), size=4, vjust=-.5) +
ylim(0, 80)
install.packages(c("GGally", "neuralnet", "NeuralNetTools"))
install.packages(c("GGally", "neuralnet", "NeuralNetTools"))
#Program: NN_Iris.r
#Author: Sanjeev adpated from
#Date: July 2020
#install.packages("neuralnet")
#install.packages("NeuralNetTools")
#install.packages("ggplot2")
#install.packages("GGally")
#install.packages("caret")
library("neuralnet")
library("NeuralNetTools")
library("ggplot2")
library("GGally")
library("caret")
data(iris)
ggplot <- function(...)
ggplot2::ggplot(...) +
scale_color_brewer(palette="Purples") +
scale_fill_brewer(palette="Purples")
unlockBinding("ggplot",parent.env(asNamespace("GGally")))
assign("ggplot",ggplot,parent.env(asNamespace("GGally")))
graph_corr <- ggpairs(iris, mapping = aes(color = Species),
columns = c('Sepal.Length',
'Sepal.Width',
'Petal.Length',
'Petal.Width',
'Species'),
columnLabels = c('Sepal.Length',
'Sepal.Width',
'Petal.Length',
'Petal.Width',
'Species'))
graph_corr <- graph_corr + theme_minimal()
graph_corr
#One of the most important procedures when forming a neural network is data normalization.
#This involves adjusting the data to a common scale so as to accurately compare predicted and actual values.
#Failure to normalize the data will typically result in the prediction value remaining the same across all observations, regardless of the input values.
#We can do this in two ways in R:
#  Scale the data frame automatically using the scale function in R
#Transform the data using a max-min normalization technique.
#In this example, will be to use Max-Min Normalization function.
norm.fun = function(x){(x - min(x))/(max(x) - min(x))}
#Apply function to normalise data()
df_iris = iris[,c("Sepal.Length","Sepal.Width",
"Petal.Length","Petal.Width" )]
df_iris = as.data.frame(apply(df_iris, 2, norm.fun))
df_iris$Species = iris$Species
df_iris$setosa <- df_iris$Species=="setosa"
df_iris$virginica <- df_iris$Species == "virginica"
df_iris$versicolor <- df_iris$Species == "versicolor"
## 75% of the sample size
smp_size <- floor(0.75 * nrow(df_iris))
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(df_iris)), size = smp_size)
training.set <- df_iris[train_ind, ]
test.set <- df_iris[-train_ind, ]
model = as.formula("Species ~
Sepal.Length +
Sepal.Width +
Petal.Length +
Petal.Width")
iris.net <- neuralnet(model,
data=training.set,
hidden=c(10,10),
rep = 5,
act.fct = "logistic",
err.fct = "ce",
linear.output = F,
lifesign = "minimal",
stepmax = 1000000,
threshold = 0.001)
plotnet(iris.net,
alpha.val = 0.8,
circle_col = list('purple', 'white', 'white'),
bord_col = 'black')
iris.prediction <- compute(iris.net, test.set)
idx <- apply(iris.prediction$net.result, 1, which.max)
predicted <- as.factor(c('setosa', 'versicolor', 'virginica')[idx])
confusionMatrix(predicted, test.set$Species)
plotnet(iris.net,
alpha.val = 0.8,
circle_col = list('purple', 'white', 'white'),
bord_col = 'black')
source('C:/Users/Public/Documents/R Scripts/DATA4800_R_NN_Iris.r')
install.packages("neuralnet")
plotnet(iris.net,
alpha.val = 0.8,
circle_col = list('purple', 'white', 'white'),
bord_col = 'black')
library("neuralnet")
library("NeuralNetTools")
library("ggplot2")
library("GGally")
library("caret")
library("neuralnet")
library("NeuralNetTools")
library("ggplot2")
library("GGally")
library("caret")
install.packages("GGally")
ggplot <- function(...)
ggplot2::ggplot(...) +
scale_color_brewer(palette="Purples") +
scale_fill_brewer(palette="Purples")
data(iris)
ggplot <- function(...)
ggplot2::ggplot(...) +
scale_color_brewer(palette="Purples") +
scale_fill_brewer(palette="Purples")
unlockBinding("ggplot",parent.env(asNamespace("GGally")))
assign("ggplot",ggplot,parent.env(asNamespace("GGally")))
graph_corr <- ggpairs(iris, mapping = aes(color = Species),
columns = c('Sepal.Length',
'Sepal.Width',
'Petal.Length',
'Petal.Width',
'Species'),
columnLabels = c('Sepal.Length',
'Sepal.Width',
'Petal.Length',
'Petal.Width',
'Species'))
graph_corr <- graph_corr + theme_minimal()
graph_corr
#One of the most important procedures when forming a neural network is data normalization.
#This involves adjusting the data to a common scale so as to accurately compare predicted and actual values.
#Failure to normalize the data will typically result in the prediction value remaining the same across all observations, regardless of the input values.
#We can do this in two ways in R:
#  Scale the data frame automatically using the scale function in R
#Transform the data using a max-min normalization technique.
#In this example, will be to use Max-Min Normalization function.
norm.fun = function(x){(x - min(x))/(max(x) - min(x))}
#Apply function to normalise data()
df_iris = iris[,c("Sepal.Length","Sepal.Width",
"Petal.Length","Petal.Width" )]
df_iris = as.data.frame(apply(df_iris, 2, norm.fun))
df_iris$Species = iris$Species
df_iris$setosa <- df_iris$Species=="setosa"
df_iris$virginica <- df_iris$Species == "virginica"
df_iris$versicolor <- df_iris$Species == "versicolor"
## 75% of the sample size
smp_size <- floor(0.75 * nrow(df_iris))
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(df_iris)), size = smp_size)
training.set <- df_iris[train_ind, ]
test.set <- df_iris[-train_ind, ]
model = as.formula("Species ~
Sepal.Length +
Sepal.Width +
Petal.Length +
Petal.Width")
iris.net <- neuralnet(model,
data=training.set,
hidden=c(10,10),
rep = 5,
act.fct = "logistic",
err.fct = "ce",
linear.output = F,
lifesign = "minimal",
stepmax = 1000000,
threshold = 0.001)
plotnet(iris.net,
alpha.val = 0.8,
circle_col = list('purple', 'white', 'white'),
bord_col = 'black')
install.packages("NeuralNetTools")
plotnet(iris.net,
alpha.val = 0.8,
circle_col = list('purple', 'white', 'white'),
bord_col = 'black')
library(NeuralNetTools)
library(neuralnet)
plotnet(iris.net,
alpha.val = 0.8,
circle_col = list('purple', 'white', 'white'),
bord_col = 'black')
iris.net <- neuralnet(model,
data=training.set,
hidden=c(10,10),
rep = 5,
act.fct = "logistic",
err.fct = "ce",
linear.output = F,
lifesign = "minimal",
stepmax = 1000000,
threshold = 0.001)
plotnet(iris.net,
alpha.val = 0.8,
circle_col = list('purple', 'white', 'white'),
bord_col = 'black')
iris.prediction <- compute(iris.net, test.set)
idx <- apply(iris.prediction$net.result, 1, which.max)
predicted <- as.factor(c('setosa', 'versicolor', 'virginica')[idx])
confusionMatrix(predicted, test.set$Species)
library(caret)
iris.prediction <- compute(iris.net, test.set)
idx <- apply(iris.prediction$net.result, 1, which.max)
predicted <- as.factor(c('setosa', 'versicolor', 'virginica')[idx])
confusionMatrix(predicted, test.set$Species)
library(randomForest)
library(randomForest)
#Program R_RF_Iris.R
# Simple Random Forest model in R to be used in week 3 of DATA 4800
#Author: Sanjeev Naguleswaran
#Date: June 2020
#install.packages("randomForest")
#install.packages("Mass")
#install.packages("caret")
library(randomForest)
library(ROCR)
library(datasets)
data(iris)
head(iris)
library(MASS)
library(caret)
# USe the set.seed function so that we get same results each time
set.seed(123)
data(iris)
View(iris)
# Splitting data into training and testing. As the species are in order
# splitting the data based on species
iris_setosa <- iris[iris$Species == "setosa", ] # 50
iris_versicolor <- iris[iris$Species == "versicolor", ] # 50
iris_virginica <- iris[iris$Species == "virginica", ] # 50
iris_train <-
rbind(iris_setosa[1:25, ], iris_versicolor[1:25, ], iris_virginica[1:25, ])
iris_test <-
rbind(iris_setosa[26:50, ], iris_versicolor[26:50, ], iris_virginica[26:50, ])
rf <- randomForest(Species ~ ., data = iris_train)
rf  # Description of the random forest with no of trees, mtry = no of variables for splitting
# each tree node.
# Out of bag estimate of error rate is 4 % in Random Forest Model.
attributes(rf)
pred1 <- predict(rf, iris_train)
head(pred1)
head(iris_train$Species)
# looks like the first six predicted value and original value matches.
confusionMatrix(pred1, iris_train$Species)  # 100 % accuracy on training data
# Around 95% Confidence Interval.
# Sensitivity for all three species/categories is 100 %
# Prediction with test data - Test Data
pred2 <- predict(rf, iris_test)
confusionMatrix(pred2, iris_test$Species) # 94,67 % accuracy on test data
# Error Rate in Random Forest Model :
plot(rf)
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf, iris_test[, -5], type = "prob")
# Use pretty colours:
pretty_colours <- c("#F8766D", "#00BA38", "#619CFF")
# Specify the different classes
classes <- levels(iris_test$Species)
# For each class
for (i in 1:3)
{
# Define which observations belong to class[i]
true_values <- ifelse(iris_test[, 5] == classes[i], 1, 0)
# Assess the performance of classifier for class[i]
pred <- prediction(prediction_for_roc_curve[, i], true_values)
perf <- performance(pred, "tpr", "fpr")
if (i == 1)
{
plot(perf, main = "ROC Curve", col = pretty_colours[i])
}
else
{
plot(perf,
main = "ROC Curve",
col = pretty_colours[i],
add = TRUE)
}
# Calculate the AUC and print it to screen
auc.perf <- performance(pred, measure = "auc")
print(auc.perf@y.values)
}
confusionMatrix(pred2, iris_test$Species) # 94,67 % accuracy on test data
#Program R_RF_Iris.R
# Simple Random Forest model in R to be used in week 3 of DATA 4800
#Author: Sanjeev Naguleswaran
#Date: June 2020
#install.packages("randomForest")
#install.packages("Mass")
#install.packages("caret")
library(randomForest)
library(ROCR)
library(datasets)
data(iris)
head(iris)
library(MASS)
library(caret)
# USe the set.seed function so that we get same results each time
set.seed(123)
data(iris)
View(iris)
# Splitting data into training and testing. As the species are in order
# splitting the data based on species
iris_setosa <- iris[iris$Species == "setosa", ] # 50
iris_versicolor <- iris[iris$Species == "versicolor", ] # 50
iris_virginica <- iris[iris$Species == "virginica", ] # 50
iris_train <-
rbind(iris_setosa[1:25, ], iris_versicolor[1:25, ], iris_virginica[1:25, ])
iris_test <-
rbind(iris_setosa[26:50, ], iris_versicolor[26:50, ], iris_virginica[26:50, ])
rf <- randomForest(Species ~ ., data = iris_train)
rf  # Description of the random forest with no of trees, mtry = no of variables for splitting
# each tree node.
# Out of bag estimate of error rate is 4 % in Random Forest Model.
attributes(rf)
pred1 <- predict(rf, iris_train)
head(pred1)
head(iris_train$Species)
# looks like the first six predicted value and original value matches.
confusionMatrix(pred1, iris_train$Species)  # 100 % accuracy on training data
# Around 95% Confidence Interval.
# Sensitivity for all three species/categories is 100 %
# Prediction with test data - Test Data
pred2 <- predict(rf, iris_test)
confusionMatrix(pred2, iris_test$Species) # 94,67 % accuracy on test data
# Error Rate in Random Forest Model :
plot(rf)
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf, iris_test[, -5], type = "prob")
# Use pretty colours:
pretty_colours <- c("#F8766D", "#00BA38", "#619CFF")
# Specify the different classes
classes <- levels(iris_test$Species)
# For each class
for (i in 1:3)
{
# Define which observations belong to class[i]
true_values <- ifelse(iris_test[, 5] == classes[i], 1, 0)
# Assess the performance of classifier for class[i]
pred <- prediction(prediction_for_roc_curve[, i], true_values)
perf <- performance(pred, "tpr", "fpr")
if (i == 1)
{
plot(perf, main = "ROC Curve", col = pretty_colours[i])
}
else
{
plot(perf,
main = "ROC Curve",
col = pretty_colours[i],
add = TRUE)
}
# Calculate the AUC and print it to screen
auc.perf <- performance(pred, measure = "auc")
print(auc.perf@y.values)
}
confusionMatrix(pred2, iris_test$Species)
install.packages("e1071")
confusionMatrix(pred2, iris_test$Species)
test_df <- confusionMatrix(pred2, iris_test$Species)
test_df
test_df$table
test_df$byClass
test_df
test_df$dots
test_df$mode
test_df$byClass
test_df$byClass$Sensitivity
test_df$byClass
test_df$byClass[0]
test_df$byClass[0:]
test_df$byClas
test_df$byCla
{
plot(perf,
main = "ROC Curve",
col = pretty_colours[i],
add = TRUE)
}
source('C:/Users/Public/Documents/R Scripts/Example_arima (1).r')
installed.packages()
source('~/.active-rstudio-document', echo=TRUE)
.libPaths()
install.packages('sets')
source('~/.active-rstudio-document', echo=TRUE)
user_packages
system_packages
packagesUsed <- c('randomForest','ROCR','datasets','MASS','caret')
intersect(packagesUsed,system_packages)
setdiff(packagesUsed,system_packages)
packagesUsed <- c('ggplot2,'cowplot','dplyr,'grid','datasets')
packagesUsed <- c('ggplot2','cowplot','dplyr,'grid','datasets')
packagesUsed <- c('ggplot2','cowplot','dplyr','grid','datasets')
setdiff(packagesUsed,system_packages)
system_packages
source('~/.active-rstudio-document', echo=TRUE)
system_packages
grepl('nlme',system_packages)
source('~/.active-rstudio-document', echo=TRUE)
system_packages
packagesUsed <- c('ggplot2','cowplot','dplyr','grid','datasets')
setdiff(packagesUsed,system_packages)
packagesUsed <- c('neuralnet','NeuralNetTools','ggplot2','GGally','caret')
setdiff(packagesUsed,system_packages)
